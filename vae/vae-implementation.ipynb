{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All discussion here will be in relation to the code for the traditional autoencoder, seen in this [ADD LINK] post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.585392Z",
     "start_time": "2018-11-23T09:22:03.735936Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42); # set seed for consistent results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the traditional autoencoder, we must specify the dimensions of the code -- here, this will correspond to the dimension of both the mean $\\mu$ and variance $\\sigma^2$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.591342Z",
     "start_time": "2018-11-23T09:22:04.588579Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dimensions = 784\n",
    "code_dimensions = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to update our definitions for the encoder and decoder network classes. The encoder network must now yield both a mean and variance vector, while the structure of the decoder network can remain relatively unchanged. (We will actually have the encoder compute the log-variance rather than the variance directly as this lets us our network output negative values; once we exponentiate these to find the proper variance, we are guaranteed to end up with positive values as desired.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.599516Z",
     "start_time": "2018-11-23T09:22:04.593169Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder class for variational autoencoder. Takes in data of dimension `input_dimensions`, \n",
    "    conducts some transformation, and outputs the resulting code consisting of two vectors of dimension `code_dimensions`,\n",
    "    the mean and log-variance vectors.  \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions: int, code_dimensions: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dimensions, (input_dimensions+code_dimensions)//2)\n",
    "        self.fc2mean = nn.Linear((input_dimensions+code_dimensions)//2, code_dimensions)\n",
    "        self.fc2logvar = nn.Linear((input_dimensions+code_dimensions)//2, code_dimensions)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        hidden1 = F.relu(self.fc1(x))\n",
    "        code_mean = self.fc2mean(hidden1)\n",
    "        code_logvar = self.fc2logvar(hidden1)\n",
    "        return code_mean, code_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.608462Z",
     "start_time": "2018-11-23T09:22:04.602607Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder class for variational autoencoder. Takes in code sample of dimension `code_dimensions`, \n",
    "    conducts some transformation, and outputs the resulting output of dimension `input_dimensions`.\n",
    "    \"\"\"\n",
    "    def __init__(self, code_dimensions: int, input_dimensions: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(code_dimensions, (input_dimensions+code_dimensions)//2)\n",
    "        self.fc2 = nn.Linear((input_dimensions+code_dimensions)//2, input_dimensions)\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        hidden1 = F.relu(self.fc1(x))\n",
    "        output = torch.sigmoid(self.fc2(hidden1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct the full VAE, we concatenate the encoder and decoder networks (as with the traditional autoencoder), but with the added step of conducting the reparametrization trick in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.617362Z",
     "start_time": "2018-11-23T09:22:04.611414Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for variational autoencoder. Composite of Encoder and Decoder classes above, with the \n",
    "    reparametrization trick in between to allow for backpropagation through a stochastic process.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dimensions: int, code_dimensions: int):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encode = Encoder(input_dimensions, code_dimensions)\n",
    "        self.decode = Decoder(code_dimensions, input_dimensions)\n",
    "        \n",
    "    def reparametrize(self, mean: torch.Tensor, logvar: torch.Tensor):\n",
    "        std_dev = torch.exp(0.5*logvar) # calculate the standard deviation for scaling\n",
    "        epsilon = torch.randn_like(std_dev) # draw vector of dimension `code_dimensions` from standard normal\n",
    "        return mean + epsilon*std_dev # return vector scaled by standard deviation and translated by mean\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        code_mean, code_logvar = self.encode(x) # from input to parametrized distribution...\n",
    "        sample = self.reparametrize(code_mean, code_logvar) # sample with reparametrization trick\n",
    "        output = self.decode(sample)\n",
    "        return output, code_mean, code_logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we aren't done yet. We're still missing an integral part of the VAE framework: the new objective function. This is composed of the reconstruction error analogous to the original autoencoder loss (for which we'll use cross-entropy loss) and the KL-divergence term describing how far our code distributions are from unit Gaussians. Conveniently, the VAE paper gives us a closed form for this instance of it (more generally, we would use `torch.distributions`, but for this simple example we have no need)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.623323Z",
     "start_time": "2018-11-23T09:22:04.619198Z"
    }
   },
   "outputs": [],
   "source": [
    "def kl_divergence(mean: torch.Tensor, logvar: torch.Tensor):\n",
    "    '''\n",
    "    Takes as input two `code_dimension`-dimensional vectors representing the mean and log-variance \n",
    "    of a Gaussian distribution and outputs the (negative) KL-divergence of this with the unit Gaussian.\n",
    "    '''\n",
    "    return -0.5 * torch.sum(1 + logvar - torch.pow(mean, 2) - torch.exp(logvar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.629274Z",
     "start_time": "2018-11-23T09:22:04.625077Z"
    }
   },
   "outputs": [],
   "source": [
    "def variational_lowerbound(input_data: torch.Tensor, output_data: torch.Tensor,\n",
    "                           code_mean: torch.Tensor, code_logvar: torch.Tensor):\n",
    "    '''\n",
    "    Returns variational lower-bound (aka evidence lower-bound) for a VAE \n",
    "    given input data, a proposed reconstruction, and the code distributions.\n",
    "    '''\n",
    "#     print('output shape:', output_data.shape, 'input_shape:', input_data.shape)\n",
    "    reconstruction_loss = F.binary_cross_entropy(output_data, input_data, reduction='sum')\n",
    "    kl_loss = kl_divergence(code_mean, code_logvar)\n",
    "    return reconstruction_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're almost ready to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through some miscellaneous setup: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.656170Z",
     "start_time": "2018-11-23T09:22:04.631180Z"
    }
   },
   "outputs": [],
   "source": [
    "# use gpu if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.661165Z",
     "start_time": "2018-11-23T09:22:04.657908Z"
    }
   },
   "outputs": [],
   "source": [
    "# some hyperparameters\n",
    "batch_sz = 32\n",
    "test_batch_sz = 100\n",
    "\n",
    "learning_rate = 10**(-3)\n",
    "\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.667372Z",
     "start_time": "2018-11-23T09:22:04.662736Z"
    }
   },
   "outputs": [],
   "source": [
    "# set dataloader kwargs\n",
    "kwargs = {'num_workers': 1, 'pin_memory':True} if torch.cuda.is_available() else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.672289Z",
     "start_time": "2018-11-23T09:22:04.668936Z"
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.707034Z",
     "start_time": "2018-11-23T09:22:04.674067Z"
    }
   },
   "outputs": [],
   "source": [
    "# create dataloaders to serve up MNIST images as example data -- see MNIST example in the official documentation \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform= img_transform),\n",
    "    batch_size=batch_sz,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:04.717808Z",
     "start_time": "2018-11-23T09:22:04.709114Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=img_transform),\n",
    "    batch_size=test_batch_sz,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the model and initializing the Adam optimizer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:07.033125Z",
     "start_time": "2018-11-23T09:22:04.719472Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VAE(input_dimensions, code_dimensions).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:07.044669Z",
     "start_time": "2018-11-23T09:22:07.035919Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: torch.optim.Optimizer, device: torch.device, \n",
    "          train_loader: torch.utils.data.dataloader.DataLoader, epoch: int):\n",
    "    \"\"\"\n",
    "    Executes one epoch of training given model, optimizer, device, dataloader, and epoch number.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (inp, label) in enumerate(train_loader):\n",
    "        inp = inp.view(-1, input_dimensions)\n",
    "        inp = inp.to(device) \n",
    "        output, code_mean, code_logvar = model(inp) # here we put the input image through the VAE\n",
    "        loss = variational_lowerbound(inp, output, code_mean, code_logvar)\n",
    "        \n",
    "        # good to check that our values are still sane and nothing has gone horribly wrong\n",
    "        if torch.isnan(loss).any() or loss.item() == np.inf or loss.item() == -np.inf:\n",
    "            print('Gradient vanished/exploded')\n",
    "            raise Exception('Died')\n",
    "            break\n",
    "        \n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # conduct step of gradient descent\n",
    "        optimizer.zero_grad() # zero out gradients to avoid messing with future iterations\n",
    "        \n",
    "        if batch_idx % 1000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(inp), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:22:07.053485Z",
     "start_time": "2018-11-23T09:22:07.047161Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model: nn.Module, device: torch.device, test_loader: torch.utils.data.dataloader.DataLoader, epoch: int):\n",
    "    \"\"\"\n",
    "    Computes error on test set given a model, device, dataloader, and epoch number.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inp, label) in enumerate(test_loader):\n",
    "            inp = inp.view(-1, input_dimensions)\n",
    "            inp = inp.to(device)\n",
    "            output, code_mean, code_logvar = model(inp)\n",
    "            test_loss += variational_lowerbound(inp, output, code_mean, code_logvar)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:23:40.755988Z",
     "start_time": "2018-11-23T09:22:07.055358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 17584.925781\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 4101.734863\n",
      "\n",
      "Test set: Average loss: 116.2248\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3681.470703\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 3519.761719\n",
      "\n",
      "Test set: Average loss: 110.8681\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 3543.985352\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 3235.022949\n",
      "\n",
      "Test set: Average loss: 108.4604\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 3841.058838\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 3662.301270\n",
      "\n",
      "Test set: Average loss: 107.3888\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 3051.623291\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 3232.370850\n",
      "\n",
      "Test set: Average loss: 106.8985\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 3346.608643\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 3394.134033\n",
      "\n",
      "Test set: Average loss: 106.2145\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 3452.067627\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 3244.714844\n",
      "\n",
      "Test set: Average loss: 105.7724\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 3192.741455\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 3323.556396\n",
      "\n",
      "Test set: Average loss: 105.6246\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 3481.543457\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 3432.222656\n",
      "\n",
      "Test set: Average loss: 105.3194\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 3492.939941\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 3406.887451\n",
      "\n",
      "Test set: Average loss: 105.3271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs+1):\n",
    "    train(model, optimizer, device, train_loader, epoch)\n",
    "    test(model, device, test_loader, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:38:28.210366Z",
     "start_time": "2018-11-23T09:38:28.205848Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_vae_samples(n: int):\n",
    "    '''\n",
    "    Function for generating n samples from the trained VAE. Currently, plots first of the n.\n",
    "    '''\n",
    "    noise_sample = torch.randn(n, code_dimensions).to(device)\n",
    "    sample = model.decode(noise_sample)\n",
    "    plt.imshow(sample.data.view(n,28,28).cpu().detach().numpy()[0], cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-23T09:39:21.535201Z",
     "start_time": "2018-11-23T09:39:21.384099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_vae_samples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
